{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5edc134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup CUDA library paths for JAX GPU support\n",
    "import os\n",
    "import sys\n",
    "import ctypes\n",
    "import glob\n",
    "\n",
    "# Find the nvidia CUDA packages - search all site-packages directories\n",
    "nvidia_base = None\n",
    "for path in sys.path:\n",
    "    potential_nvidia = os.path.join(path, 'nvidia')\n",
    "    if os.path.exists(potential_nvidia) and os.path.isdir(potential_nvidia):\n",
    "        nvidia_base = potential_nvidia\n",
    "        print(f\"Found nvidia packages at: {nvidia_base}\")\n",
    "        break\n",
    "\n",
    "if nvidia_base:\n",
    "    lib_dirs = glob.glob(f\"{nvidia_base}/*/lib\")\n",
    "    \n",
    "    if lib_dirs:\n",
    "        current_ld_path = os.environ.get('LD_LIBRARY_PATH', '')\n",
    "        new_ld_path = ':'.join(lib_dirs)\n",
    "        if current_ld_path:\n",
    "            new_ld_path = f\"{new_ld_path}:{current_ld_path}\"\n",
    "        os.environ['LD_LIBRARY_PATH'] = new_ld_path\n",
    "        \n",
    "        preloaded = []\n",
    "        for lib_dir in lib_dirs:\n",
    "            for lib_name in ['libcudart.so.12', 'libcublas.so.12', 'libcublasLt.so.12']:\n",
    "                lib_path = os.path.join(lib_dir, lib_name)\n",
    "                if os.path.exists(lib_path):\n",
    "                    try:\n",
    "                        ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)\n",
    "                        preloaded.append(lib_name)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning loading {lib_name}: {e}\")\n",
    "        \n",
    "        print(f\"âœ“ Set LD_LIBRARY_PATH with {len(lib_dirs)} CUDA directories\")\n",
    "        print(f\"âœ“ Preloaded {len(set(preloaded))} CUDA libraries\")\n",
    "    else:\n",
    "        print(f\"âš  Found nvidia directory but no lib subdirectories\")\n",
    "else:\n",
    "    print(\"âš  Could not find nvidia CUDA packages in sys.path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d70e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import jax\n",
    "print(\"JAX version:\", jax.__version__)\n",
    "print(\"Available devices:\", jax.devices())\n",
    "print(\"Default backend:\", jax.default_backend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f63314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from jax import numpy as jp\n",
    "from brax import envs\n",
    "from brax.io import html\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.training.agents.ppo import networks as ppo_networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243b0599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural Walking Humanoid Environment\n",
    "\n",
    "from brax.envs.base import PipelineEnv, State\n",
    "\n",
    "class NaturalWalkingHumanoid(PipelineEnv):\n",
    "    \"\"\"Humanoid environment with rewards shaped for natural human-like walking.\n",
    "    \n",
    "    Key improvements for natural gait:\n",
    "    1. Target velocity reward (penalize being too fast OR too slow)\n",
    "    2. ARM SWING reward (arms should swing opposite to legs)\n",
    "    3. FOOT SPACING reward (feet shoulder-width apart, not on a line)\n",
    "    4. Upright posture reward (keep torso vertical)\n",
    "    5. Smoothness reward (penalize jerky motions)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        target_velocity: float = 1.4,      # Normal human walking ~1.4 m/s\n",
    "        velocity_reward_weight: float = 2.0,\n",
    "        forward_reward_weight: float = 0.5,\n",
    "        ctrl_cost_weight: float = 0.1,\n",
    "        healthy_reward: float = 3.0,\n",
    "        posture_reward_weight: float = 1.0,\n",
    "        smoothness_reward_weight: float = 0.3,\n",
    "        arm_swing_reward_weight: float = 1.5,\n",
    "        foot_spacing_reward_weight: float = 1.0,\n",
    "        terminate_when_unhealthy: bool = True,\n",
    "        healthy_z_range: tuple = (0.8, 2.0),\n",
    "        reset_noise_scale: float = 1e-2,\n",
    "        backend: str = 'generalized',\n",
    "        **kwargs,\n",
    "    ):\n",
    "        from brax.io import mjcf\n",
    "        from etils import epath\n",
    "        \n",
    "        path = epath.resource_path('brax') / 'envs/assets/humanoid.xml'\n",
    "        sys = mjcf.load(path)\n",
    "        \n",
    "        n_frames = 5\n",
    "        if backend in ['spring', 'positional']:\n",
    "            sys = sys.tree_replace({'opt.timestep': 0.0015})\n",
    "            n_frames = 10\n",
    "            \n",
    "        kwargs['n_frames'] = kwargs.get('n_frames', n_frames)\n",
    "        super().__init__(sys=sys, backend=backend, **kwargs)\n",
    "        \n",
    "        self._target_velocity = target_velocity\n",
    "        self._velocity_reward_weight = velocity_reward_weight\n",
    "        self._forward_reward_weight = forward_reward_weight\n",
    "        self._ctrl_cost_weight = ctrl_cost_weight\n",
    "        self._healthy_reward = healthy_reward\n",
    "        self._posture_reward_weight = posture_reward_weight\n",
    "        self._smoothness_reward_weight = smoothness_reward_weight\n",
    "        self._arm_swing_reward_weight = arm_swing_reward_weight\n",
    "        self._foot_spacing_reward_weight = foot_spacing_reward_weight\n",
    "        self._terminate_when_unhealthy = terminate_when_unhealthy\n",
    "        self._healthy_z_range = healthy_z_range\n",
    "        self._reset_noise_scale = reset_noise_scale\n",
    "        \n",
    "        # Body part indices\n",
    "        self._torso_idx = 0\n",
    "        self._right_foot_idx = 5\n",
    "        self._left_foot_idx = 8\n",
    "        self._right_upper_arm_idx = 9\n",
    "        self._left_upper_arm_idx = 11\n",
    "        self._right_thigh_idx = 3\n",
    "        self._left_thigh_idx = 6\n",
    "        \n",
    "    def reset(self, rng: jax.Array) -> State:\n",
    "        rng, rng1, rng2 = jax.random.split(rng, 3)\n",
    "        \n",
    "        low, hi = -self._reset_noise_scale, self._reset_noise_scale\n",
    "        qpos = self.sys.init_q + jax.random.uniform(\n",
    "            rng1, (self.sys.q_size(),), minval=low, maxval=hi\n",
    "        )\n",
    "        qvel = jax.random.uniform(\n",
    "            rng2, (self.sys.qd_size(),), minval=low, maxval=hi\n",
    "        )\n",
    "        \n",
    "        pipeline_state = self.pipeline_init(qpos, qvel)\n",
    "        obs = self._get_obs(pipeline_state, jp.zeros(self.sys.act_size()))\n",
    "        \n",
    "        reward, done, zero = jp.zeros(3)\n",
    "        metrics = {\n",
    "            'forward_reward': zero,\n",
    "            'velocity_reward': zero,\n",
    "            'posture_reward': zero,\n",
    "            'smoothness_reward': zero,\n",
    "            'arm_swing_reward': zero,\n",
    "            'foot_spacing_reward': zero,\n",
    "            'reward_ctrl': zero,\n",
    "            'reward_alive': zero,\n",
    "            'x_position': zero,\n",
    "            'x_velocity': zero,\n",
    "            'distance_from_origin': zero,\n",
    "        }\n",
    "        return State(pipeline_state, obs, reward, done, metrics)\n",
    "    \n",
    "    def step(self, state: State, action: jax.Array) -> State:\n",
    "        action_min = self.sys.actuator.ctrl_range[:, 0]\n",
    "        action_max = self.sys.actuator.ctrl_range[:, 1]\n",
    "        action = (action + 1) * (action_max - action_min) * 0.5 + action_min\n",
    "        \n",
    "        pipeline_state0 = state.pipeline_state\n",
    "        pipeline_state = self.pipeline_step(pipeline_state0, action)\n",
    "        \n",
    "        com_before = self._com(pipeline_state0)\n",
    "        com_after = self._com(pipeline_state)\n",
    "        velocity = (com_after - com_before) / self.dt\n",
    "        x_velocity = velocity[0]\n",
    "        \n",
    "        body_pos = pipeline_state.x.pos\n",
    "        body_vel = pipeline_state.xd.vel\n",
    "        \n",
    "        # 1. Forward reward\n",
    "        forward_reward = self._forward_reward_weight * x_velocity\n",
    "        \n",
    "        # 2. Velocity targeting reward\n",
    "        velocity_error = jp.abs(x_velocity - self._target_velocity)\n",
    "        velocity_reward = self._velocity_reward_weight * jp.exp(-velocity_error**2 / 0.5)\n",
    "        \n",
    "        # 3. Posture reward\n",
    "        torso_z = body_pos[self._torso_idx, 2]\n",
    "        torso_upright = jp.clip(torso_z - 1.0, 0, 0.5) / 0.5\n",
    "        posture_reward = self._posture_reward_weight * torso_upright\n",
    "        \n",
    "        # 4. Smoothness reward\n",
    "        smoothness_reward = self._smoothness_reward_weight\n",
    "        \n",
    "        # 5. ARM SWING REWARD\n",
    "        right_arm_vel_x = body_vel[self._right_upper_arm_idx, 0]\n",
    "        left_arm_vel_x = body_vel[self._left_upper_arm_idx, 0]\n",
    "        right_leg_vel_x = body_vel[self._right_thigh_idx, 0]\n",
    "        left_leg_vel_x = body_vel[self._left_thigh_idx, 0]\n",
    "        \n",
    "        arm_leg_opposition = (\n",
    "            -right_arm_vel_x * right_leg_vel_x +\n",
    "            -left_arm_vel_x * left_leg_vel_x\n",
    "        )\n",
    "        arm_swing_reward = self._arm_swing_reward_weight * jp.tanh(arm_leg_opposition * 0.5)\n",
    "        \n",
    "        # 6. FOOT SPACING REWARD\n",
    "        right_foot_y = body_pos[self._right_foot_idx, 1]\n",
    "        left_foot_y = body_pos[self._left_foot_idx, 1]\n",
    "        foot_spacing = jp.abs(right_foot_y - left_foot_y)\n",
    "        \n",
    "        target_spacing = 0.2\n",
    "        spacing_error = jp.abs(foot_spacing - target_spacing)\n",
    "        foot_spacing_reward = self._foot_spacing_reward_weight * jp.exp(-spacing_error**2 / 0.02)\n",
    "        \n",
    "        # 7. Control cost\n",
    "        ctrl_cost = self._ctrl_cost_weight * jp.sum(jp.square(action))\n",
    "        \n",
    "        # 8. Healthy reward\n",
    "        min_z, max_z = self._healthy_z_range\n",
    "        is_healthy = jp.where(body_pos[self._torso_idx, 2] < min_z, 0.0, 1.0)\n",
    "        is_healthy = jp.where(body_pos[self._torso_idx, 2] > max_z, 0.0, is_healthy)\n",
    "        \n",
    "        healthy_reward = self._healthy_reward if self._terminate_when_unhealthy else self._healthy_reward * is_healthy\n",
    "        \n",
    "        reward = (\n",
    "            forward_reward + velocity_reward + posture_reward + smoothness_reward +\n",
    "            arm_swing_reward + foot_spacing_reward + healthy_reward - ctrl_cost\n",
    "        )\n",
    "        \n",
    "        done = 1.0 - is_healthy if self._terminate_when_unhealthy else 0.0\n",
    "        obs = self._get_obs(pipeline_state, action)\n",
    "        \n",
    "        state.metrics.update(\n",
    "            forward_reward=forward_reward,\n",
    "            velocity_reward=velocity_reward,\n",
    "            posture_reward=posture_reward,\n",
    "            smoothness_reward=smoothness_reward,\n",
    "            arm_swing_reward=arm_swing_reward,\n",
    "            foot_spacing_reward=foot_spacing_reward,\n",
    "            reward_ctrl=-ctrl_cost,\n",
    "            reward_alive=healthy_reward,\n",
    "            x_position=com_after[0],\n",
    "            x_velocity=x_velocity,\n",
    "            distance_from_origin=jp.linalg.norm(com_after),\n",
    "        )\n",
    "        \n",
    "        return state.replace(\n",
    "            pipeline_state=pipeline_state, obs=obs, reward=reward, done=done\n",
    "        )\n",
    "    \n",
    "    def _com(self, pipeline_state) -> jax.Array:\n",
    "        inertia = self.sys.link.inertia\n",
    "        mass_sum = jp.sum(inertia.mass)\n",
    "        x_i = pipeline_state.x.vmap().do(inertia.transform)\n",
    "        com = jp.sum(jax.vmap(jp.multiply)(inertia.mass, x_i.pos), axis=0) / mass_sum\n",
    "        return com\n",
    "    \n",
    "    def _get_obs(self, pipeline_state, action) -> jax.Array:\n",
    "        position = pipeline_state.q[2:]\n",
    "        velocity = pipeline_state.qd\n",
    "        \n",
    "        com, inertia, mass_sum, x_i = self._com_full(pipeline_state)\n",
    "        cinr = x_i.replace(pos=x_i.pos - com).vmap().do(inertia)\n",
    "        com_inertia = jp.hstack(\n",
    "            [cinr.i.reshape((cinr.i.shape[0], -1)), inertia.mass[:, None]]\n",
    "        )\n",
    "        \n",
    "        from brax.base import Transform\n",
    "        xd_i = (\n",
    "            Transform.create(pos=x_i.pos - pipeline_state.x.pos)\n",
    "            .vmap()\n",
    "            .do(pipeline_state.xd)\n",
    "        )\n",
    "        com_vel = inertia.mass[:, None] * xd_i.vel / mass_sum\n",
    "        com_ang = xd_i.ang\n",
    "        com_velocity = jp.hstack([com_vel, com_ang])\n",
    "        \n",
    "        from brax import actuator\n",
    "        qfrc_actuator = actuator.to_tau(\n",
    "            self.sys, action, pipeline_state.q, pipeline_state.qd\n",
    "        )\n",
    "        \n",
    "        return jp.concatenate([\n",
    "            position, velocity, com_inertia.ravel(), com_velocity.ravel(),\n",
    "            qfrc_actuator\n",
    "        ])\n",
    "    \n",
    "    def _com_full(self, pipeline_state):\n",
    "        inertia = self.sys.link.inertia\n",
    "        mass_sum = jp.sum(inertia.mass)\n",
    "        x_i = pipeline_state.x.vmap().do(inertia.transform)\n",
    "        com = jp.sum(jax.vmap(jp.multiply)(inertia.mass, x_i.pos), axis=0) / mass_sum\n",
    "        return com, inertia, mass_sum, x_i\n",
    "\n",
    "\n",
    "# Register and create the environment\n",
    "envs._envs['natural_humanoid'] = NaturalWalkingHumanoid\n",
    "\n",
    "env = NaturalWalkingHumanoid(\n",
    "    target_velocity=1.4,\n",
    "    velocity_reward_weight=2.0,\n",
    "    forward_reward_weight=0.5,\n",
    "    ctrl_cost_weight=0.1,\n",
    "    healthy_reward=3.0,\n",
    "    posture_reward_weight=1.0,\n",
    "    smoothness_reward_weight=0.3,\n",
    "    arm_swing_reward_weight=1.5,\n",
    "    foot_spacing_reward_weight=1.0,\n",
    "    backend='generalized'\n",
    ")\n",
    "\n",
    "print(\"âœ… Created Natural Walking Humanoid Environment\")\n",
    "print(f\"   Target velocity: 1.4 m/s\")\n",
    "print(f\"   Observation size: {env.observation_size}\")\n",
    "print(f\"   Action size: {env.action_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1248553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for Natural Walking\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸš¶ Training for NATURAL WALKING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def make_networks_factory(obs_shape, action_size, preprocess_observations_fn=lambda x: x):\n",
    "    return ppo_networks.make_ppo_networks(\n",
    "        obs_shape, action_size,\n",
    "        preprocess_observations_fn=preprocess_observations_fn,\n",
    "        policy_hidden_layer_sizes=(128, 128, 128, 128),\n",
    "        value_hidden_layer_sizes=(128, 128, 128, 128),\n",
    "    )\n",
    "\n",
    "print(\"\\nTraining started...\")\n",
    "train_fn = lambda: ppo.train(\n",
    "    environment=env,\n",
    "    num_timesteps=50_000_000,\n",
    "    num_evals=20,\n",
    "    reward_scaling=0.1,\n",
    "    episode_length=1500,\n",
    "    normalize_observations=True,\n",
    "    action_repeat=1,\n",
    "    unroll_length=20,\n",
    "    num_minibatches=32,\n",
    "    num_updates_per_batch=4,\n",
    "    discounting=0.99,\n",
    "    learning_rate=3e-4,\n",
    "    entropy_cost=5e-3,\n",
    "    num_envs=512,\n",
    "    batch_size=512,\n",
    "    network_factory=make_networks_factory,\n",
    ")\n",
    "\n",
    "inference_fn, params, metrics = train_fn()\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… Training Complete!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d733cde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Natural Walking Gait\n",
    "\n",
    "import numpy as np\n",
    "from jax import numpy as jnp\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸš¶ Visualizing Natural Walking Gait\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "env_vis = NaturalWalkingHumanoid(target_velocity=1.4, backend='generalized')\n",
    "eval_policy = inference_fn(params, deterministic=True)\n",
    "\n",
    "jit_reset = jax.jit(env_vis.reset)\n",
    "jit_step = jax.jit(env_vis.step)\n",
    "jit_policy = jax.jit(eval_policy)\n",
    "\n",
    "def compute_com(pipeline_state):\n",
    "    inertia = env_vis.sys.link.inertia\n",
    "    mass_sum = jnp.sum(inertia.mass)\n",
    "    x_i = pipeline_state.x.vmap().do(inertia.transform)\n",
    "    com = jnp.sum(jax.vmap(jnp.multiply)(inertia.mass, x_i.pos), axis=0) / mass_sum\n",
    "    return com\n",
    "\n",
    "jit_compute_com = jax.jit(compute_com)\n",
    "dt = env_vis.dt\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "state = jit_reset(rng)\n",
    "states = []\n",
    "\n",
    "episode_length = 1000\n",
    "record_every = 2\n",
    "\n",
    "previous_com = jit_compute_com(state.pipeline_state)\n",
    "com_velocities = []\n",
    "com_positions = []\n",
    "\n",
    "print(f\"Running {episode_length} steps...\")\n",
    "\n",
    "for step in range(episode_length):\n",
    "    if step % 200 == 0:\n",
    "        com = jit_compute_com(state.pipeline_state)\n",
    "        avg_vel = np.mean(com_velocities) if com_velocities else 0\n",
    "        print(f\"  Step {step}/{episode_length}, COM: {float(com[0]):.2f}m, Avg vel: {avg_vel:.2f} m/s\")\n",
    "    \n",
    "    rng, key = jax.random.split(rng)\n",
    "    act, _ = jit_policy(state.obs, key)\n",
    "    state = jit_step(state, act)\n",
    "    \n",
    "    current_com = jit_compute_com(state.pipeline_state)\n",
    "    com_velocity = (current_com - previous_com) / dt\n",
    "    com_velocities.append(float(com_velocity[0]))\n",
    "    com_positions.append(float(current_com[0]))\n",
    "    previous_com = current_com\n",
    "    \n",
    "    if step % record_every == 0:\n",
    "        states.append(state.pipeline_state)\n",
    "    \n",
    "    if state.done:\n",
    "        print(f\"  Episode ended at step {step}\")\n",
    "        break\n",
    "\n",
    "# Analysis\n",
    "final_distance = com_positions[-1] if com_positions else 0\n",
    "avg_velocity = np.mean(com_velocities) if com_velocities else 0\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Š Natural Walking Analysis\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Distance traveled:   {final_distance:.2f} m\")\n",
    "print(f\"Average velocity:    {avg_velocity:.2f} m/s\")\n",
    "print(f\"Target velocity:     1.40 m/s\")\n",
    "print(f\"Velocity accuracy:   {avg_velocity/1.4*100:.1f}% of target\")\n",
    "\n",
    "# Save HTML\n",
    "output_path = \"/mnt/c/Users/makis/Documents/GitHub/braxphysics/RL/humanoid_natural_walk.html\"\n",
    "print(f\"\\nRendering HTML ({len(states)} frames)...\")\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(html.render(env_vis.sys, states))\n",
    "\n",
    "print(f\"âœ“ Saved: {output_path}\")\n",
    "print(\"\\nðŸŽ‰ Open humanoid_natural_walk.html in a browser to see the result!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2986ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training Metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"ðŸ“Š Training Metrics Summary\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('PPO Training - Final Metrics Summary', fontsize=16)\n",
    "\n",
    "# Extract scalar metrics\n",
    "scalar_metrics = {}\n",
    "for key, values in metrics.items():\n",
    "    if hasattr(values, '__array__'):\n",
    "        values = np.asarray(values)\n",
    "    if isinstance(values, np.ndarray):\n",
    "        if values.ndim == 0 or values.size == 1:\n",
    "            scalar_metrics[key] = float(values)\n",
    "    else:\n",
    "        scalar_metrics[key] = values\n",
    "\n",
    "# Plot 1: Episode Performance\n",
    "ax1 = axes[0, 0]\n",
    "performance_metrics = {\n",
    "    'Episode\\nReward': scalar_metrics.get('eval/episode_reward', 0),\n",
    "    'Episode\\nLength': scalar_metrics.get('eval/avg_episode_length', 0),\n",
    "    'X Position': scalar_metrics.get('eval/episode_x_position', 0),\n",
    "}\n",
    "bars1 = ax1.bar(range(len(performance_metrics)), list(performance_metrics.values()), color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "ax1.set_xticks(range(len(performance_metrics)))\n",
    "ax1.set_xticklabels(list(performance_metrics.keys()))\n",
    "ax1.set_title('Episode Performance')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: Reward Components  \n",
    "ax2 = axes[0, 1]\n",
    "reward_components = {\n",
    "    'Forward': scalar_metrics.get('eval/episode_forward_reward', 0),\n",
    "    'Alive': scalar_metrics.get('eval/episode_reward_alive', 0),\n",
    "    'Control': scalar_metrics.get('eval/episode_reward_quadctrl', 0),\n",
    "}\n",
    "bars2 = ax2.bar(range(len(reward_components)), list(reward_components.values()), color=['#2ca02c', '#9467bd', '#d62728'])\n",
    "ax2.set_xticks(range(len(reward_components)))\n",
    "ax2.set_xticklabels(list(reward_components.keys()))\n",
    "ax2.set_title('Reward Components')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: Training Losses\n",
    "ax3 = axes[1, 0]\n",
    "losses = {\n",
    "    'Policy': abs(scalar_metrics.get('training/policy_loss', 0)),\n",
    "    'Value': scalar_metrics.get('training/v_loss', 0),\n",
    "    'Entropy': abs(scalar_metrics.get('training/entropy_loss', 0)),\n",
    "}\n",
    "ax3.bar(range(len(losses)), list(losses.values()), color=['#ff7f0e', '#8c564b', '#e377c2'])\n",
    "ax3.set_xticks(range(len(losses)))\n",
    "ax3.set_xticklabels(list(losses.keys()))\n",
    "ax3.set_title('Final Training Losses')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 4: Speed Metrics\n",
    "ax4 = axes[1, 1]\n",
    "speed_metrics = {\n",
    "    'X Velocity': scalar_metrics.get('eval/episode_x_velocity', 0),\n",
    "    'Distance': scalar_metrics.get('eval/episode_distance_from_origin', 0),\n",
    "}\n",
    "ax4.bar(range(len(speed_metrics)), list(speed_metrics.values()), color=['#bcbd22', '#17becf'])\n",
    "ax4.set_xticks(range(len(speed_metrics)))\n",
    "ax4.set_xticklabels(list(speed_metrics.keys()))\n",
    "ax4.set_title('Movement Metrics')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_metrics_reward_shaping.png', dpi=150)\n",
    "print(\"âœ“ Saved training metrics to 'training_metrics_reward_shaping.png'\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
